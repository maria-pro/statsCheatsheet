# Cheatsheet on common stats

# Linear regression:


## standard error 
"typical" distance between the estimated and the "true" coefficient.

** standard deviaion of the sampling distribution of the estimate of the coefficient (e.g. standard error of variable X)

** an estimate of the variance of the strength of the effect, 
= the strength of the relationship between each causal variable and the predicted variable. 

**High standard errors**: you can't estimate the coefficient precisely, the 'true' coefficient is very far away from  the estimated value 

## t-statistic

* the coefficient divided by its standard error
* a measure of the precision with which the regression coefficient is measured.
* an estimate of how extreme the value you see is, relative to the standard error 

## p-value 

* an estimate of the probability of seeing a t-value as unusual as the one you obtained, if the null hypothesis were true. 

* low p-value: all is good = effect is present, low chances to obtain this coefficient, if the relationship does not exist (the coefficient is 0)

## null hypothesis 

* is usually "no effect", unless something else is specified = the true coefficient is zero

* if the p-value is very low, then there is a higher probability that you're seeing data that is counter-indicative of zero effect. In other situations, you can get a p-value based on other statistics and variables.

# Diagnostic of regression
## fitted.values

`model$fitted.values`: PRED	= Unstandardized predicted values.

ZPRED = Standardized predicted values.
`scale(x, center = TRUE, scale = TRUE)` = `scale(model$fitted.values)`

**Check**: Linearity, Homogeneity of Error Variance = scale(model$fitted.values) 

## Residuals

`model$residuals`: RESID = Unstandardized residuals.	

ZRESID	Standardized residuals.

**Check**: Linearity, Homogeneity of Error Variance, Outliers

Standardized variables (either the predicted values or the residuals) have a mean of zero and standard deviation of one. If residuals are normally distributed, then 95% of them should fall between -2 and 2. If they fall above 2 or below -2, they can be considered unusual.

# LEVER	Centered leverage values.	Unusual and Influential Data

## Additional measures

`cooks.distance(model)`: Cook's distance (hat statistics)

* calculated for each observation by comparing the results of the regression with that observation included to the results when that particular observation is removed. 



**Check**: Unusual and Influential Data


COOK	Cook’s distances.
DFBETA	DF Beta


Residual standard error: 37780000 on 693 degrees of freedom
  (10 observations deleted due to missingness)
Multiple R-squared:  0.009204,	Adjusted R-squared:  -0.01367 
F-statistic: 0.4024 on 16 and 693 DF,  p-value: 0.9821


## tests
### Testing Nonlinearity: 
* the relationship between the response variable and the predictors is linear. 


Keyword	Description	Assumption Checked




the null hypothesis is that the true coefficient is zero; if that probability is low, it's suggesting that it would be rare to get a result as unusual as this if the coefficient were really zero.

***
## Residuals

* The difference between the observed value of the dependent variable (y) and the predicted value (ŷ) is called the residual (e). Each data point has one residual. Both the sum and the mean of the residuals are equal to zero.

<

      Min        1Q    Median        3Q       Max 
